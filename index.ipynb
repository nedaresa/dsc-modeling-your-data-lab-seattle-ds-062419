{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Your Data - Lab\n",
    "\n",
    "## Introduction \n",
    "\n",
    "In this lab you'll perform a full linear regression on the data. You'll implement the process demonstrated in the previous lesson, taking a stepwise approach to analyze and improve the model along the way.\n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "\n",
    "* Remove predictors with p-values too high and refit the model\n",
    "* Examine and interpret the model results\n",
    "* Split data into training and testing sets\n",
    "* Fit a regression model to the data set using statsmodel library\n",
    "\n",
    "\n",
    "## Build an Initial Regression Model\n",
    "\n",
    "To start, perform a train-test split and create an initial regression model to model the `list_price` using all of your available features.\n",
    "\n",
    "> **Note:** In order to write the model you'll have to do some tedious manipulation of your column names. Statsmodels will not allow you to have spaces, apostrophe or arithmetic symbols (+) in your column names. Preview them and refine them as you go.  \n",
    "**If you receive an error such as \"PatsyError: error tokenizing input (maybe an unclosed string?)\" then you need to further preprocess your column names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Lego_dataset_cleaned.csv')\n",
    "print(list(df.columns)[0:10])\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "def col_format(df):\n",
    "    corrected_cols=[]\n",
    "    for i in df.columns:\n",
    "        i=i.replace(' ', '_')\n",
    "        i=i.replace('-', '_')\n",
    "        i=i.replace('+', 'plus')\n",
    "        i=i.replace(\"'\", \"\")\n",
    "        i=i.replace('.', '')\n",
    "        i=i.replace('™', '')\n",
    "        i=i.replace('®', '')\n",
    "        i=i.replace('½', 'half')\n",
    "        corrected_cols.append(i)\n",
    "    return corrected_cols\n",
    "col_format(df)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=col_format(df)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subs = [(' ', '_'),('.',''),(\"'\",\"\"),('™', ''), ('®',''),('+','plus'), ('½','half'), ('-','_')]\n",
    "# def col_formatting(col):\n",
    "#     for old, new in subs:\n",
    "#         col = col.replace(old,new)\n",
    "#     return col\n",
    "\n",
    "# df.columns = [col_formatting(col) for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the Uninfluential Features\n",
    "\n",
    "Based on the initial model, remove those features which do not appear to be statistically relevant and rerun the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n",
    "predictors=list(df.drop(labels='list_price', axis=1).columns)\n",
    "predictors= '+'.join(predictors)\n",
    "outcome='list_price'\n",
    "formula=outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=train).fit()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Comment:** You should see that the model performance is identical. Additionally, observe that there are further features which have been identified as unimpactful. Continue to refine the model accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = model.summary()\n",
    "p_table = summary.tables[1]\n",
    "p_table = pd.DataFrame(p_table.data)\n",
    "p_table.columns = p_table.iloc[0]\n",
    "p_table = p_table.drop(0)\n",
    "p_table = p_table.set_index(p_table.columns[0])\n",
    "p_table['P>|t|'] = p_table['P>|t|'].astype(float)\n",
    "x = list(p_table[p_table['P>|t|']<0.05].index)\n",
    "x.remove('Intercept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refit model with subset features\n",
    "predictors = '+'.join(x)\n",
    "formula = outcome + \"~\" + predictors\n",
    "model = ols(formula=formula, data=train).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the p-value table from the summary and use it to subset our features\n",
    "summary = model.summary()\n",
    "\n",
    "p_table = summary.tables[1]\n",
    "p_table = pd.DataFrame(p_table.data)\n",
    "p_table.columns = p_table.iloc[0]\n",
    "p_table = p_table.drop(0)\n",
    "p_table = p_table.set_index(p_table.columns[0])\n",
    "p_table['P>|t|'] = p_table['P>|t|'].astype(float)\n",
    "x = list(p_table[p_table['P>|t|']<0.05].index)\n",
    "x.remove('Intercept')\n",
    "p_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refit model with subset features\n",
    "predictors = '+'.join(x)\n",
    "formula = outcome + \"~\" + predictors\n",
    "model = ols(formula=formula, data=train).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Multicollinearity\n",
    "\n",
    "There are still a lot of features in the current model! Chances are there are some strong multicollinearity issues. Begin to investigate the extend of this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X = df[x]\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "list(zip(x, vif))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Another Round of Feature Selection\n",
    "\n",
    "Once again, subset your features based on your findings above. Then rerun the model once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "vif_scores = list(zip(x, vif))\n",
    "x = [x for x,vif in vif_scores if vif < 5]\n",
    "print(len(vif_scores), len(x_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = '+'.join(x)\n",
    "formula = outcome + \"~\" + predictors\n",
    "model = ols(formula=formula, data=train).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Normality Assumption\n",
    "\n",
    "Check whether the normality assumption holds for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import statsmodels.api as sm\n",
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Homoscedasticity Assumption\n",
    "\n",
    "Check whether the model's errors are indeed homoscedastic or if they violate this principle and display heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "plt.scatter(model.predict(train[x]), model.resid)\n",
    "plt.plot(model.predict(train[x]), [0 for i in range(len(train))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Comment:** This displays a fairly pronounced 'funnel' shape: errors appear to increase as the list_price increases. This doesn't bode well for our model. Subsetting the data to remove outliers and confiding the model to this restricted domain may be necessary. A log transformation or something equivalent may also be appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Additional Refinements\n",
    "\n",
    "From here, make additional refinements to your model based on the above analysis. As you progress, continue to go back and check the assumptions for the updated model. Be sure to attempt at least 2 additional model refinements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Comment:** Based on the above plots, it seems as though outliers are having a substantial impact on the model. As such, removing outliers may be appropriate. Investigating the impact of a log transformation is also worthwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "df.list_price.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(80,100):\n",
    "    q = i/100\n",
    "    print(\"{} percentile: {}\".format(q, df.list_price.quantile(q=q)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_tot = len(df)\n",
    "df = df[df.list_price < 450] #Subsetting to remove extreme outliers\n",
    "print('Percent removed:', (orig_tot -len(df))/orig_tot)\n",
    "df.list_price = df.list_price.map(np.log) #Applying a log transformation\n",
    "train, test = train_test_split(df)\n",
    "\n",
    "#Refit model with subset features\n",
    "predictors = '+'.join(x)\n",
    "formula = outcome + \"~\" + predictors\n",
    "model = ols(formula=formula, data=train).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment: There is still a clear underestimate now by the model as the log transformed \n",
    "# list price increases, but the model is much improved. Further subsetting is potentially warranted.\n",
    "plt.scatter(model.predict(train[x]), model.resid)\n",
    "plt.plot(model.predict(train[x]), [0 for i in range(len(train))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! As you can see, regression can be a challenging task that requires you to make decisions along the way, try alternative approaches and make ongoing refinements. These choices depend on the context and specific use cases. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
